<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title> Skeleton2Point: Recognizing Skeleton-Based Actions as Point Clouds </title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Skeleton2Point: Recognizing Skeleton-Based Actions as Point Clouds</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=p8ZtdScAAAAJ&hl=zh-CN" target="_blank">Baiqiao Yin</a><sup>12</sup>,</span>
                <span class="author-block">
                  Jiaying Lin</a><sup>12</sup>,</span>
                <span class="author-block">
                  Jiajun Wen</a><sup>12</sup>,</span>
                <span class="author-block">
                  Yue Li</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://github.com/liujf69" target="_blank">Jinfu Liu</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.ece.pku.edu.cn/info/1046/2596.htm" target="_blank">Mengyuan Liu</a><sup>2</sup><sup>&#10013</sup>
                  </span>
                  </div>
                  <div>
                    <sup>&#10013</sup> Corresponding Author.
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Sun Yat-sen University<sup>1</sup>, National Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School<sup>2</sup>, <br>2024</span>

                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width has-text-justified">
        <h2 class="title is-3 has-text-centered">
          Framework of our proposed Skeleton2Point
        </h2>
        <div class="content has-text-centered">
          <img
            src="./skeleton2point_image/MM一版_00.png"
            class="inline-figure-six"
            alt="Humans can perform spatial reasoning while VLMs doesn't."
          />
        </div>
        <div class="content has-text-justified">
          <p>
            <b>Framework:</b> We propose a skeleton-to-point net-
            work (Skeleton2Point) that consists of two trunk branches. In the
            first branch, referred to as the human skeleton branch, skeleton data
            is encoded with given space-time information and then fed into
            a graph transformer neural network to obtain predictions. In the
            second branch, regarded as the point cloud branch, skeleton data is
            transformed into point clouds form with an information transform
            module. Next, FPS and kNN are used to sample and model the posi-
            tion relations between the points, then a point cloud information
            extractor is leveraged to extract latent features. We also propose a
            Cluster-Dispatch-based interaction module to enhance the discrim-
            ination of local-global information. 
          </p>
        </div>
        <h3 class="title is-4"></h3>
      
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Skeleton-based action recognition has achieved remarkable results by developing graph convolutional networks (GCNs) and skeleton transformers. However, the existing methods pay much more attention to encoding joints' position with the given time and serial number information, neglecting to model the positional information contained in the 3D coordinate channel itself. To solve these problems, this paper proposes a skeleton-to-point network <b>(Skeleton2Point)</b> to model joints' position relationships in three-dimensional space, which is the first to leverage point cloud methods into skeleton-based action recognition in a dual-learner approach. The human skeleton learner feeds compact skeletal representations in the skeleton transformer network, which is composed of a spatial transformer block and a temporal transformer block. In the point cloud learner, skeleton data is transformed into point clouds form with a proposed <b>I</b>nformation <b>T</b>ransform <b>M</b>odule (ITM), which fills the channel information with the spatial and temporal serial number. Then, several point cloud learning levels are adopted to extract deep position features. The point cloud learning level is made of three key layers: Sampling layer, Grouping layer, and Point cloud
extract layer. We also propose a <b>C</b>luster-<b>D</b>ispatch-based <b>I</b>nteraction module (CDI) to enhance the discrimination of local-global information. In comparison with existing methods on NTU-RGB+D 60 and NTU-RGB+D 120 datasets, Skeleton2Point achieves SOTA levels on both joint modality and stream fusion. Especially, on the challenging NTU-RGB+D 120 dataset under the X-Sub and X-Set setting, the accuracies reach 90.63% and 91.86%
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width has-text-justified">
        <h2 class="title is-3 has-text-centered">
          Motivation of our Skeleton2Point
        </h2>
        <div class="content has-text-centered">
          <img
            src="./skeleton2point_image/图1_00.png"
            class="inline-figure-six"
            alt="Humans can perform spatial reasoning while VLMs doesn't."
          />
        </div>
        <div class="content has-text-justified">
          <p>
            <b>Motivation:</b> Containing 3D coordinates, skeleton joints can be naturally
            viewed as point clouds distributed in three-dimensional space as
            illustrated. However, while dealing with skeleton joint
            coordinates in skeleton data, all these methods pay much more
            attention to the topological relationships that exist between the
            joints, encoding joints' position with the given time and serial num-
            ber information, neglecting to model the positional information
            contained in the 3D coordinates channel itself.
          </p>
        </div>
        <h3 class="title is-4"></h3>
        <div class="content has-text-justified">
          <p>
            <b>Key insight:</b> To our best knowledge, we are the first to regard skeleton
            joints as point clouds via incorporating the position information of skeletons into point cloud methods, demonstrating
            the validity of modeling position relationships with 3D coordinates.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-justified">
        <h2 class="title is-3 has-text-centered">Experiments</h2>

        <div class="content has-text-justified">
          <p>
            Through extensive benchmark, we found our proposed framework can
            significantly enhance the ability of skeleton recognition models in
            performing different types of actions.
          </p>
        </div>
        <h3 class="title is-4">Comparison against state-of-the-art methods</h3>

        <div class="content has-text-justified">
          <p>
            We adopt PointConT as the
            lite point cloud information extractor and PointMLP as the heavy point
            cloud information extractor and compared with the state-
            of-the-art methods on two different datasets: NTU-RGB+D 60 and
            NTU-RGB+D 120 datasets to verify the
            competitive performance. Especially, for a fair comparison, we also used joint stream dataset
            only, because joints contain 3D coordinates
            the same as point clouds. If the bone or motion stream datasets
            were used in point cloud learner, it couldn't be well-compatible.
          </p>
        </div>
        <div class="content has-text-centered">

          <img
            src="./skeleton2point_image/MM表1.png"
            class="inline-figure-six"
            alt="table for comparison with baselines"
            height="auto"
            width="800px"
          />
        </div>


        <h3 class="title is-4">Ablation Studies</h3>


      </div>
    </div>
  </div>
</section>
<!--tables-->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">

      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="./skeleton2point_image/表2.png" width=900 height=300>
        <h2 class="subtitle has-text-centered">
          Ablation studies of point cloud branch on the NTU-
RGB+D 60 and NTU-RGB+D 120 datasets with the joint input
modality in point cloud branch.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img  src="./skeleton2point_image/表3.png" width=900 height=300>
        <h2 class="subtitle has-text-centered">
          Ablation studies of point cloud branch on the NTU-
RGB+D 60 dataset with the joint input modality in point
cloud branch.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="./skeleton2point_image/表4.png" width=1200 height=300>
        <h2 class="subtitle has-text-centered">
          Comparison of parameter, computation cost when
          training&inferring and accuracy on the NTU-RGB+D 60 and
          NTU-RGB+D 120 cross-subject Protocols.
       </h2>
     </div>
          


    <div style="text-align: center; width: 900px; ">
      <!-- Your image here -->
      <img src="./skeleton2point_image/表5.png" width=900 height="auto" class="inline-figure-six" style="display: inline-block;" />
      <h2 class="subtitle has-text-centered">
        Comparison of different combinations of 𝛼 and 𝛽 when ensembling skeleton learner and lite point cloud
learner in joint modality.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
            
            <img src="./skeleton2point_image/表6.png" width=900 height="auto" class="inline-figure-six" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Comparison of different sampling and grouping
methods in NTU-RGB+D 60 under the X-Sub setting.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">
        Visualization
      </h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <iframe src="./skeleton2point_image/vis1.pdf" width=900 height=900></iframe>
        <h2 class="subtitle has-text-centered">
           Visualization of different actions (two persons) in the forms of skeleton and point clouds.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <iframe src="./skeleton2point_image/vis2.pdf" width=900 height=900></iframe>
        <h2 class="subtitle has-text-centered">
          Visualization of different actions (single person) in
the forms of skeleton and point clouds.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <iframe src="./skeleton2point_image/vis3.pdf" width=900 height=900></iframe>
        <h2 class="subtitle has-text-centered">
         Visualization of point clouds during the down sample process.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="./skeleton2point_image/class2.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Visualization of the top-15 actions with the highest
change when the human skeleton branch is integrated with
the point cloud branch for NTU-RGB+D 120 dataset under
the X-Sub setting with the joint input modality
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->










<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="./skeleton2point_image/Skeleton2Point_Recognizing Skeleton Based Actions As Point.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->




<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

